import torch
import torch.nn as nn
import torch.nn.functional as F


class LogitAdjust(nn.Module):
    def __init__(self, cls_num_list, tau=1, weight=None):
        super(LogitAdjust, self).__init__()
        cls_num_list = torch.cuda.FloatTensor(cls_num_list)
        cls_p_list = cls_num_list / cls_num_list.sum()
        m_list = tau * torch.log(cls_p_list)
        self.m_list = m_list.view(1, -1)
        self.weight = weight

    def forward(self, x, target):
        x_m = x + self.m_list
        return F.cross_entropy(x_m, target, weight=self.weight)


class LogitAdjust_soft(nn.Module):  
    def __init__(self, cls_num_list, tau=1, weight=None):  
        super(LogitAdjust_soft, self).__init__()  
        cls_num_list = torch.cuda.FloatTensor(cls_num_list)  
        cls_p_list = cls_num_list / cls_num_list.sum()  
        m_list = tau * torch.log(cls_p_list)  
        self.m_list = m_list.view(1, -1)  
        self.weight = weight  
  
    def forward(self, x, labels_update):  
        x_m = x + self.m_list  
        return F.kl_div(F.log_softmax(x_m, dim=1), labels_update, reduction='batchmean')  

class LA_KD(nn.Module):
    def __init__(self, cls_num_list, tau=1):
        super(LA_KD, self).__init__()
        cls_num_list = torch.cuda.FloatTensor(cls_num_list)
        cls_p_list = cls_num_list / cls_num_list.sum()
        m_list = tau * torch.log(cls_p_list)
        self.m_list = m_list.view(1, -1)

    def forward(self, x, target, soft_target, w_kd):
        x_m = x + self.m_list
        log_pred = torch.log_softmax(x_m, dim=-1)
        log_pred = torch.where(torch.isinf(log_pred), torch.full_like(log_pred, 0), log_pred)

        kl = F.kl_div(log_pred, soft_target, reduction='batchmean')

        return w_kd * kl + (1 - w_kd) * F.nll_loss(log_pred, target)

